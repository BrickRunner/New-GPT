import asyncio
import ollama
import os
from dotenv import load_dotenv
from aiogram import Bot, Dispatcher
from aiogram.types import Message
from aiogram.filters import CommandStart
from concurrent.futures import ThreadPoolExecutor


# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
executor = ThreadPoolExecutor()
load_dotenv()
bot = Bot(token=os.getenv('TG_TOKEN'))
dp = Dispatcher()


# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Ollama —Å —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º
def generate_text(prompt: str) -> str:
    response = ollama.chat(
        model="mistral:7b", 
        messages=[
            {"role": "system", "content": "–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –±—ã—Å—Ç—Ä–æ."},
            {"role": "user", "content": prompt}
        ]  
    )
    return response['message']['content']


# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start
@dp.message(CommandStart())
async def start(message: Message):
    await message.answer("–ü—Ä–∏–≤–µ—Ç! –Ø Dmitry_GPT. –ù–∞–ø–∏—à–∏ —á—Ç–æ-–Ω–∏–±—É–¥—å, –∏ —è –æ—Ç–≤–µ—á—É!")


# –û—Ç–≤–µ—Ç –Ω–∞ –ª—é–±–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —á–µ—Ä–µ–∑ AI
@dp.message()
async def chat_with_ai(message: Message):
    user_text = message.text
    await message.answer("‚è≥ –î—É–º–∞—é...")

    loop = asyncio.get_running_loop()
    ai_response = await loop.run_in_executor(executor, generate_text, user_text)
    await message.answer(ai_response)


# –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
async def main():
    print("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω!")
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())


